{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanjitp/CS4225/blob/main/cs4225_a2_colab_KanJitpakdi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Spark SQL (15m)"
      ],
      "metadata": {
        "id": "c9TcaCrlWvhN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkbrHZYEw5Cr"
      },
      "outputs": [],
      "source": [
        "# Setup Spark\n",
        "# ===============\n",
        "# Installing Spark needs to be done once each time you re-open this notebook. It should take around 10-30 seconds.\n",
        "# ===============\n",
        "# install java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After downloading dataset, you should have the files in your Files (click the folder icon in the left sidebar)\n",
        "!wget -O Products_table.csv https://drive.google.com/uc?id=1FG0rGWSPWALcmFo3feHUF5TK5AP7mMwH&export=download #products\n",
        "!wget -O Sales_table.csv https://drive.google.com/uc?id=1l1fr_s67JjGGsXt3fIz_769pKPZg-jhU&export=download #sales\n",
        "!wget -O Sellers_table.csv https://drive.google.com/uc?id=1YTTYU5Cwgvau1Z7b1ShmcIhrO3VN-Zhq&export=download #sellers"
      ],
      "metadata": {
        "id": "2luSAeOXxBiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e30a8804-ec81-40b9-fee4-75695a05b472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-01 07:44:20--  https://drive.google.com/uc?id=1FG0rGWSPWALcmFo3feHUF5TK5AP7mMwH\n",
            "Resolving drive.google.com (drive.google.com)... 172.253.62.102, 172.253.62.101, 172.253.62.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.253.62.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0g-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ab6p3ngbtar5an2on9cmgujhd9g8fv8s/1680335025000/06948221057362969045/*/1FG0rGWSPWALcmFo3feHUF5TK5AP7mMwH?uuid=f06e982f-d973-436c-9393-57d6a308ac28 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-04-01 07:44:22--  https://doc-0g-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ab6p3ngbtar5an2on9cmgujhd9g8fv8s/1680335025000/06948221057362969045/*/1FG0rGWSPWALcmFo3feHUF5TK5AP7mMwH?uuid=f06e982f-d973-436c-9393-57d6a308ac28\n",
            "Resolving doc-0g-a0-docs.googleusercontent.com (doc-0g-a0-docs.googleusercontent.com)... 142.251.16.132, 2607:f8b0:4004:c17::84\n",
            "Connecting to doc-0g-a0-docs.googleusercontent.com (doc-0g-a0-docs.googleusercontent.com)|142.251.16.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2430503 (2.3M) [text/csv]\n",
            "Saving to: ‘Products_table.csv’\n",
            "\n",
            "Products_table.csv  100%[===================>]   2.32M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-04-01 07:44:22 (172 MB/s) - ‘Products_table.csv’ saved [2430503/2430503]\n",
            "\n",
            "--2023-04-01 07:44:22--  https://drive.google.com/uc?id=1l1fr_s67JjGGsXt3fIz_769pKPZg-jhU\n",
            "Resolving drive.google.com (drive.google.com)... 172.253.62.102, 172.253.62.101, 172.253.62.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.253.62.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0s-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2jkiojsvrjm3hhndv404o40lq13qiu9n/1680335025000/06948221057362969045/*/1l1fr_s67JjGGsXt3fIz_769pKPZg-jhU?uuid=5dcf93bd-7049-4f0c-87a4-9cca0ccc97b8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-04-01 07:44:23--  https://doc-0s-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2jkiojsvrjm3hhndv404o40lq13qiu9n/1680335025000/06948221057362969045/*/1l1fr_s67JjGGsXt3fIz_769pKPZg-jhU?uuid=5dcf93bd-7049-4f0c-87a4-9cca0ccc97b8\n",
            "Resolving doc-0s-a0-docs.googleusercontent.com (doc-0s-a0-docs.googleusercontent.com)... 142.251.16.132, 2607:f8b0:4004:c17::84\n",
            "Connecting to doc-0s-a0-docs.googleusercontent.com (doc-0s-a0-docs.googleusercontent.com)|142.251.16.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6957214 (6.6M) [text/csv]\n",
            "Saving to: ‘Sales_table.csv’\n",
            "\n",
            "Sales_table.csv     100%[===================>]   6.63M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-04-01 07:44:24 (47.6 MB/s) - ‘Sales_table.csv’ saved [6957214/6957214]\n",
            "\n",
            "--2023-04-01 07:44:24--  https://drive.google.com/uc?id=1YTTYU5Cwgvau1Z7b1ShmcIhrO3VN-Zhq\n",
            "Resolving drive.google.com (drive.google.com)... 172.253.62.102, 172.253.62.101, 172.253.62.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.253.62.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-14-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/o2lee08rjsa23g6qqfq0h7vir2j7pdp9/1680335025000/06948221057362969045/*/1YTTYU5Cwgvau1Z7b1ShmcIhrO3VN-Zhq?uuid=dd170bd2-f770-4e66-be3e-6594ffa83dfd [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-04-01 07:44:24--  https://doc-14-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/o2lee08rjsa23g6qqfq0h7vir2j7pdp9/1680335025000/06948221057362969045/*/1YTTYU5Cwgvau1Z7b1ShmcIhrO3VN-Zhq?uuid=dd170bd2-f770-4e66-be3e-6594ffa83dfd\n",
            "Resolving doc-14-a0-docs.googleusercontent.com (doc-14-a0-docs.googleusercontent.com)... 142.251.16.132, 2607:f8b0:4004:c17::84\n",
            "Connecting to doc-14-a0-docs.googleusercontent.com (doc-14-a0-docs.googleusercontent.com)|142.251.16.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1077818 (1.0M) [text/csv]\n",
            "Saving to: ‘Sellers_table.csv’\n",
            "\n",
            "Sellers_table.csv   100%[===================>]   1.03M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-04-01 07:44:24 (57.8 MB/s) - ‘Sellers_table.csv’ saved [1077818/1077818]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read csv files into dataframes, you can work with the 3 tables after running this code\n",
        "products_table = spark.read.option('header', True).option('inferSchema', True).csv(\"/content/Products_table.csv\").repartition(1).cache()\n",
        "sales_table = spark.read.option('header', True).option('inferSchema', True).csv(\"/content/Sales_table.csv\").repartition(1).cache()\n",
        "sellers_table = spark.read.option('header', True).option('inferSchema', True).csv(\"/content/Sellers_table.csv\").repartition(1).cache()"
      ],
      "metadata": {
        "id": "haSMnjoBxCOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products_table"
      ],
      "metadata": {
        "id": "90QH4t_XNXM0",
        "outputId": "6df55ddf-0770-4faf-ae1b-38525803da43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[product_id: int, product_name: string, price: int]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sales_table"
      ],
      "metadata": {
        "id": "zV9-Vp5BN73e",
        "outputId": "b7efca13-2931-4f64-8c97-0b85ce133457",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[order_id: int, product_id: int, seller_id: int, num_of_items_sold: int]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sellers_table"
      ],
      "metadata": {
        "id": "eieThlgaN8IZ",
        "outputId": "97d632f4-4ae1-4e8e-fee0-1621a9f68f7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[seller_id: int, seller_name: string, rating: int]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add all imports\n",
        "from pyspark.sql.functions import sum, asc, desc, col"
      ],
      "metadata": {
        "id": "WQLSDTxaQ6j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (a) Output the top 3 most popular products sold among all sellers [2m]\n",
        "# Your table should have 1 column(s): [product_name]\n",
        "\n",
        "top_3_products = (\n",
        "    sales_table\n",
        "    .groupBy('product_id')\n",
        "    .sum('num_of_items_sold')\n",
        "    .withColumnRenamed('sum(num_of_items_sold)', 'total_items_sold')\n",
        "    .join(products_table, 'product_id')\n",
        "    .select('product_name')\n",
        "    .orderBy(col('total_items_sold').desc())\n",
        "    .limit(3)\n",
        ")\n",
        "\n",
        "top_3_products.show()"
      ],
      "metadata": {
        "id": "vccDBf_CxC0a",
        "outputId": "4e028445-7369-426b-8c8e-dc13a785aef5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "| product_name|\n",
            "+-------------+\n",
            "|product_51270|\n",
            "|product_18759|\n",
            "|product_59652|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (b) Find out the total sales of the products sold by sellers 1 to 10 and output the top most sold product [2m]\n",
        "# Your table should have 1 column(s): [product_name]\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# filter the sellers for seller 1 to 10\n",
        "filtered_sellers = sales_table.filter(col('seller_id').between(1, 10))\n",
        "\n",
        "top_product_for_seller_1_to_10 = (\n",
        "    filtered_sellers\n",
        "    .groupBy('product_id')\n",
        "    .agg(sum('num_of_items_sold').alias('total_items_sold'))\n",
        "    .join(products_table, 'product_id')\n",
        "    .select('product_name')\n",
        "    .orderBy(desc('total_items_sold'))\n",
        "    .limit(1)\n",
        ")\n",
        "\n",
        "top_product_for_seller_1_to_10.show()"
      ],
      "metadata": {
        "id": "Ljmb_1OaxC8Q",
        "outputId": "92f27d96-e776-4c23-b413-b7cf56bf5d9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "| product_name|\n",
            "+-------------+\n",
            "|product_36658|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (c) Compute the combined revenue earned from sellers where seller_id ranges from 1 to 500 inclusive. [3m]\n",
        "# Your table should have 1 column(s): [total_revenue]\n",
        "\n",
        "revenue = (\n",
        "    sales_table\n",
        "    .filter(col('seller_id').between(1, 500))\n",
        "    .join(products_table, 'product_id')\n",
        "    .withColumn('revenue', col('num_of_items_sold') * col('price'))\n",
        "    .agg(sum('revenue').alias('total_revenue'))\n",
        ")\n",
        "\n",
        "revenue.show()"
      ],
      "metadata": {
        "id": "QtinRRycxDBS",
        "outputId": "75d25a3d-ea65-47bc-a2fe-48f62d23b228",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|total_revenue|\n",
            "+-------------+\n",
            "|    160916699|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (d) Among sellers with rating >= 4 who have achieved a combined number of products sold >= 3000, find out the top 10 most expensive product sold by any of the sellers. (If there are multiple products at the same price, please sort them in ascending order of product_id) [8m]\n",
        "# Your table should have 1 column(s): [product_name]\n",
        "# To get the full mark, your query should not run for more than 1 min\n",
        "\n",
        "high_rating_sellers = sellers_table.filter(col('rating') >= 4)\n",
        "\n",
        "# sellers with rating >= 4 who have achieved a combined number of products sold >= 3000\n",
        "sales_high_rating_sellers = (\n",
        "    sales_table\n",
        "    .join(high_rating_sellers, 'seller_id')\n",
        "    .groupBy('seller_id')\n",
        "    .agg(sum('num_of_items_sold').alias('total_items_sold'))\n",
        "    .filter(col('total_items_sold') >= 3000)\n",
        ")\n",
        "\n",
        "# For video illustration\n",
        "sales_high_rating_sellers.show(10)\n",
        "\n",
        "# sellers and their product that they sold\n",
        "seller_with_product_sold = (\n",
        "    sales_high_rating_sellers\n",
        "    .join(sales_table, 'seller_id')\n",
        ")\n",
        "\n",
        "# For video illustration\n",
        "seller_with_product_sold.show(10)\n",
        "\n",
        "# top 10 most expensive product sold by any of the sellers.\n",
        "# If there are multiple products at the same price, please sort them in ascending order of product_id)\n",
        "top_expensive_products = (\n",
        "    seller_with_product_sold\n",
        "    .join(products_table, 'product_id')\n",
        "    .select('product_name')\n",
        "    .orderBy(desc('price'), asc('product_id'))\n",
        "    .limit(10)\n",
        ")\n",
        "\n",
        "# Final Result\n",
        "top_expensive_products.show(10)\n"
      ],
      "metadata": {
        "id": "S-iyIoUKxada",
        "outputId": "6bbe70e1-f1c9-4083-a533-cd6a1470515d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------------+\n",
            "|seller_id|total_items_sold|\n",
            "+---------+----------------+\n",
            "|    23608|            6285|\n",
            "|    34610|            4441|\n",
            "|     1543|            4147|\n",
            "|    10269|            3071|\n",
            "|    38703|            3897|\n",
            "|    35872|            4955|\n",
            "|    39620|            5147|\n",
            "|    31478|            3384|\n",
            "|    16139|            3577|\n",
            "|    13309|            4448|\n",
            "+---------+----------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+---------+----------------+--------+----------+-----------------+\n",
            "|seller_id|total_items_sold|order_id|product_id|num_of_items_sold|\n",
            "+---------+----------------+--------+----------+-----------------+\n",
            "|    23608|            6285|  293756|     95179|              673|\n",
            "|    23608|            6285|  175321|     51037|              531|\n",
            "|    23608|            6285|  149971|     49213|              840|\n",
            "|    23608|            6285|  147765|     16749|              898|\n",
            "|    23608|            6285|  128426|     51717|               39|\n",
            "|    23608|            6285|   65018|     40479|              415|\n",
            "|    23608|            6285|   49187|     66413|              365|\n",
            "|    23608|            6285|   46291|     96847|              740|\n",
            "|    23608|            6285|   22816|     95632|              934|\n",
            "|    23608|            6285|       3|     76927|              850|\n",
            "+---------+----------------+--------+----------+-----------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+------------+\n",
            "|product_name|\n",
            "+------------+\n",
            "| product_106|\n",
            "| product_117|\n",
            "| product_363|\n",
            "| product_712|\n",
            "| product_712|\n",
            "| product_843|\n",
            "| product_897|\n",
            "| product_897|\n",
            "| product_923|\n",
            "|product_1466|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Spark ML (10m)"
      ],
      "metadata": {
        "id": "f_mZhcusW0-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Spark\n",
        "# ===============\n",
        "# Installing Spark needs to be done once each time you re-open this notebook. It should take around 10-30 seconds.\n",
        "# ===============\n",
        "# install java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "HxxRailSWpb0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After downloading dataset, you should have the files in your Files (click the folder icon in the left sidebar)\n",
        "!wget -O bank_train.csv https://drive.google.com/uc?id=1kEP94BfULB3gUMl_IQCg9wuX4IJRajMC&export=download #products\n",
        "!wget -O bank_test.csv https://drive.google.com/uc?id=1EqX4liL5iWbwqyJ_lFaYvYZvgBFwpwSJ&export=download #bank_test"
      ],
      "metadata": {
        "id": "qCqPHSerW_al",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e4b5942-9831-4779-b804-dae8a854de81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-03 02:33:52--  https://drive.google.com/uc?id=1kEP94BfULB3gUMl_IQCg9wuX4IJRajMC\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.134.113, 74.125.134.139, 74.125.134.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.134.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0c-a8-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/i36gk5ej68jfuvk62o3hchvn40io6fdr/1680489225000/08487103376102314083/*/1kEP94BfULB3gUMl_IQCg9wuX4IJRajMC?uuid=518284f8-595a-45c4-8da6-ccc579d9cb03 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-04-03 02:33:53--  https://doc-0c-a8-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/i36gk5ej68jfuvk62o3hchvn40io6fdr/1680489225000/08487103376102314083/*/1kEP94BfULB3gUMl_IQCg9wuX4IJRajMC?uuid=518284f8-595a-45c4-8da6-ccc579d9cb03\n",
            "Resolving doc-0c-a8-docs.googleusercontent.com (doc-0c-a8-docs.googleusercontent.com)... 172.217.204.132, 2607:f8b0:400c:c15::84\n",
            "Connecting to doc-0c-a8-docs.googleusercontent.com (doc-0c-a8-docs.googleusercontent.com)|172.217.204.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 730948 (714K) [text/csv]\n",
            "Saving to: ‘bank_train.csv’\n",
            "\n",
            "bank_train.csv      100%[===================>] 713.82K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-04-03 02:33:53 (103 MB/s) - ‘bank_train.csv’ saved [730948/730948]\n",
            "\n",
            "--2023-04-03 02:33:53--  https://drive.google.com/uc?id=1EqX4liL5iWbwqyJ_lFaYvYZvgBFwpwSJ\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.134.113, 74.125.134.139, 74.125.134.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.134.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-a8-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/q80jor4nmvvkrb5fdj21dpcerhdehhv2/1680489225000/08487103376102314083/*/1EqX4liL5iWbwqyJ_lFaYvYZvgBFwpwSJ?uuid=0a9ecea4-8cc0-4b39-b397-66f9f8015255 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-04-03 02:33:53--  https://doc-08-a8-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/q80jor4nmvvkrb5fdj21dpcerhdehhv2/1680489225000/08487103376102314083/*/1EqX4liL5iWbwqyJ_lFaYvYZvgBFwpwSJ?uuid=0a9ecea4-8cc0-4b39-b397-66f9f8015255\n",
            "Resolving doc-08-a8-docs.googleusercontent.com (doc-08-a8-docs.googleusercontent.com)... 172.217.204.132, 2607:f8b0:400c:c15::84\n",
            "Connecting to doc-08-a8-docs.googleusercontent.com (doc-08-a8-docs.googleusercontent.com)|172.217.204.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 182844 (179K) [text/csv]\n",
            "Saving to: ‘bank_test.csv’\n",
            "\n",
            "bank_test.csv       100%[===================>] 178.56K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-04-03 02:33:53 (87.6 MB/s) - ‘bank_test.csv’ saved [182844/182844]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bank_train_location = \"/content/bank_train.csv\"\n",
        "bank_test_location = \"/content/bank_test.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "bank_train = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(bank_train_location)\n",
        "\n",
        "bank_test = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(bank_test_location)"
      ],
      "metadata": {
        "id": "VV27jT7wWoHc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build ML model to predict whether the customer will subscribe bank deposit service or not. Train the model using training set and evaluate the model performance (e.g. accuracy) using testing set.\n",
        "\n",
        "\n",
        "*   You can explore different methods to pre-process the data and select proper features\n",
        "*   You can explore different methods to pre-process the data and select proper features\n",
        "*   Present the final testing accuracy."
      ],
      "metadata": {
        "id": "fIoXrjaq5OG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In this task I will train 3 separate ML algorithm and tune their hyperparameters to find the \n",
        "# best model for each of them through hyper parameter tuning\n",
        "\n",
        "# data preparation (4m)\n",
        "\n",
        "# Converting string (aka categorial data) to numerical data\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, ChiSqSelector\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# List of categorical columns\n",
        "categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "\n",
        "# Create a list of indexers and encoders for categorical columns\n",
        "# assignes unique index to each distinct string value\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_index\") for column in categorical_columns]\n",
        "# assign index 0 or 1 to the indexed string \n",
        "encoders = [OneHotEncoder(inputCol=column + \"_index\", outputCol=column + \"_vec\") for column in categorical_columns]\n",
        "\n",
        "# Define the feature columns to be used in the model\n",
        "feature_columns = [column + \"_vec\" for column in categorical_columns] + ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
        "\n",
        "# Create a VectorAssembler to combine all feature columns into a single vector\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# Create a pipeline to process the data\n",
        "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
        "\n",
        "# Fit the pipeline to the training data and transform the data\n",
        "prepared_train_data = pipeline.fit(bank_train).transform(bank_train)\n",
        "prepared_test_data = pipeline.fit(bank_test).transform(bank_test)\n",
        "\n",
        "# Selecting most relevant features\n",
        "# Initialize the ChiSqSelector\n",
        "# Higher dependence between target label and variables indicate that we can use it to predict the other variable\n",
        "selector = ChiSqSelector(numTopFeatures=15, featuresCol=\"features\", outputCol=\"selected_features\", labelCol=\"label\")\n",
        "\n",
        "# Fit the selector to the prepared_train_data\n",
        "selector_model = selector.fit(prepared_train_data)\n",
        "\n",
        "# Transform the train and test datasets with the selected features\n",
        "prepared_train_data = selector_model.transform(prepared_train_data)\n",
        "prepared_test_data = selector_model.transform(prepared_test_data)\n",
        "\n",
        "prepared_train_data.show(5)\n",
        "prepared_test_data.show(5)"
      ],
      "metadata": {
        "id": "tMW6Ltcr5J06",
        "outputId": "2975c47d-668e-4f78-9a6b-65a2b3f1c7f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-------+---------+-------+-------+-------+----+---------+---+-----+--------+--------+-----+--------+--------+-----+---------+-------------+---------------+-------------+-------------+----------+-------------+-----------+--------------+---------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+-------------+--------------------+--------------------+\n",
            "|age|       job|marital|education|default|balance|housing|loan|  contact|day|month|duration|campaign|pdays|previous|poutcome|label|job_index|marital_index|education_index|default_index|housing_index|loan_index|contact_index|month_index|poutcome_index|        job_vec|  marital_vec|education_vec|  default_vec|  housing_vec|     loan_vec|  contact_vec|     month_vec| poutcome_vec|            features|   selected_features|\n",
            "+---+----------+-------+---------+-------+-------+-------+----+---------+---+-----+--------+--------+-----+--------+--------+-----+---------+-------------+---------------+-------------+-------------+----------+-------------+-----------+--------------+---------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+-------------+--------------------+--------------------+\n",
            "| 45|    admin.|married|  unknown|     no|   2033|     no|  no| cellular| 28|  may|      48|       4|   -1|       0| unknown|    0|      3.0|          0.0|            3.0|          0.0|          0.0|       0.0|          0.0|        0.0|           0.0| (11,[3],[1.0])|(2,[0],[1.0])|    (3,[],[])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])|(11,[0],[1.0])|(3,[0],[1.0])|(42,[3,11,16,17,1...|(15,[5,6,7,9,14],...|\n",
            "| 56|    admin.|married|  primary|     no|    202|    yes|  no|  unknown|  9|  may|     178|       2|   -1|       0| unknown|    0|      3.0|          0.0|            2.0|          0.0|          1.0|       0.0|          1.0|        0.0|           0.0| (11,[3],[1.0])|(2,[0],[1.0])|(3,[2],[1.0])|(1,[0],[1.0])|    (1,[],[])|(1,[0],[1.0])|(2,[1],[1.0])|(11,[0],[1.0])|(3,[0],[1.0])|(42,[3,11,15,16,1...|(15,[6,8,9,14],[1...|\n",
            "| 50| housemaid| single|secondary|     no|    799|     no|  no|telephone| 28|  jan|      63|       1|   -1|       0| unknown|    0|     10.0|          1.0|            0.0|          0.0|          0.0|       0.0|          2.0|        8.0|           0.0|(11,[10],[1.0])|(2,[1],[1.0])|(3,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|    (2,[],[])|(11,[8],[1.0])|(3,[0],[1.0])|(42,[10,12,13,16,...|(15,[3,5,6,14],[1...|\n",
            "| 58|    admin.|married|secondary|     no|   1464|    yes| yes|  unknown|  5|  jun|      53|      29|   -1|       0| unknown|    0|      3.0|          0.0|            0.0|          0.0|          1.0|       1.0|          1.0|        3.0|           0.0| (11,[3],[1.0])|(2,[0],[1.0])|(3,[0],[1.0])|(1,[0],[1.0])|    (1,[],[])|    (1,[],[])|(2,[1],[1.0])|(11,[3],[1.0])|(3,[0],[1.0])|(42,[3,11,13,16,2...|(15,[8,14],[1.0,1...|\n",
            "| 43|management| single| tertiary|     no|  11891|     no|  no| cellular|  4|  dec|     821|       5|  242|       1| success|    1|      0.0|          1.0|            1.0|          0.0|          0.0|       0.0|          0.0|       11.0|           2.0| (11,[0],[1.0])|(2,[1],[1.0])|(3,[1],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])|    (11,[],[])|(3,[2],[1.0])|(42,[0,12,14,16,1...|(15,[3,4,5,6,7],[...|\n",
            "+---+----------+-------+---------+-------+-------+-------+----+---------+---+-----+--------+--------+-----+--------+--------+-----+---------+-------------+---------------+-------------+-------------+----------+-------------+-----------+--------------+---------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+-------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+-----+---------+-------------+---------------+-------------+-------------+----------+-------------+-----------+--------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+---------------+-------------+--------------------+--------------------+\n",
            "|age|        job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|label|job_index|marital_index|education_index|default_index|housing_index|loan_index|contact_index|month_index|poutcome_index|       job_vec|  marital_vec|education_vec|  default_vec|  housing_vec|     loan_vec|  contact_vec|      month_vec| poutcome_vec|            features|   selected_features|\n",
            "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+-----+---------+-------------+---------------+-------------+-------------+----------+-------------+-----------+--------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+---------------+-------------+--------------------+--------------------+\n",
            "| 45| management|married| tertiary|     no|   2220|    yes|  no|cellular| 11|  jul|     128|       2|   -1|       0| unknown|    0|      0.0|          0.0|            1.0|          0.0|          1.0|       0.0|          0.0|        2.0|           0.0|(11,[0],[1.0])|(2,[0],[1.0])|(3,[1],[1.0])|(1,[0],[1.0])|    (1,[],[])|(1,[0],[1.0])|(2,[0],[1.0])| (11,[2],[1.0])|(3,[0],[1.0])|(42,[0,11,14,16,1...|(15,[4,6,7,14],[1...|\n",
            "| 36|blue-collar| single|secondary|     no|   3623|     no|  no| unknown| 12|  nov|      71|       1|  378|       1| success|    1|      1.0|          1.0|            0.0|          0.0|          0.0|       0.0|          1.0|        5.0|           2.0|(11,[1],[1.0])|(2,[1],[1.0])|(3,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])| (11,[5],[1.0])|(3,[2],[1.0])|(42,[1,12,13,16,1...|(15,[0,3,5,6,8,10...|\n",
            "| 37| management|married|  primary|     no|   1506|     no|  no|cellular|  2|  nov|     101|       3|   80|       3| success|    0|      0.0|          0.0|            2.0|          0.0|          0.0|       0.0|          0.0|        5.0|           2.0|(11,[0],[1.0])|(2,[0],[1.0])|(3,[2],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])| (11,[5],[1.0])|(3,[2],[1.0])|(42,[0,11,15,16,1...|(15,[5,6,7,10],[1...|\n",
            "| 65|     admin.|married|secondary|     no|    952|     no|  no|cellular|  6|  sep|     255|       1|   96|       1| success|    1|      3.0|          0.0|            0.0|          0.0|          0.0|       0.0|          0.0|       10.0|           2.0|(11,[3],[1.0])|(2,[0],[1.0])|(3,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])|(11,[10],[1.0])|(3,[2],[1.0])|(42,[3,11,13,16,1...|(15,[5,6,7,13],[1...|\n",
            "| 37| management|married| tertiary|     no|     40|     no|  no|cellular| 27|  aug|    1033|       4|   -1|       0| unknown|    1|      0.0|          0.0|            1.0|          0.0|          0.0|       0.0|          0.0|        1.0|           0.0|(11,[0],[1.0])|(2,[0],[1.0])|(3,[1],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])| (11,[1],[1.0])|(3,[0],[1.0])|(42,[0,11,14,16,1...|(15,[4,5,6,7,14],...|\n",
            "+---+-----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+-----+---------+-------------+---------------+-------------+-------------+----------+-------------+-----------+--------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+---------------+-------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model building (4m)\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.classification import RandomForestClassifier,  LogisticRegression, GBTClassifier\n",
        "from pyspark.sql.functions import sum, asc, desc, col\n",
        "\n",
        "# Create and train the models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50, maxDepth=10),\n",
        "    'Logistic Regression': LogisticRegression(featuresCol=\"features\", labelCol=\"label\"),\n",
        "    'Gradient-Boosted Trees': GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=50, maxDepth=5)\n",
        "}\n",
        "\n",
        "trained_models = {name: model.fit(prepared_train_data) for name, model in models.items()}\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Print result of different models\n",
        "for name, model in trained_models.items():\n",
        "    predictions = model.transform(prepared_test_data)\n",
        "    auc = evaluator.evaluate(predictions)\n",
        "    accuracy = predictions.filter(predictions.label == predictions.prediction).count() / predictions.count()\n",
        "    print(f\"{name} - AUC: {auc:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# Result:\n",
        "# Logistic Regression - AUC: 0.8754, Accuracy: 0.7895\n",
        "# Random Forest - AUC: 0.9031, Accuracy: 0.8249\n",
        "# Gradient-Boosted Trees - AUC: 0.8949, Accuracy: 0.8186    "
      ],
      "metadata": {
        "id": "PaULiAZv5Npd",
        "outputId": "dc0828a1-952f-40af-a9ef-71fb1dee3c4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest - AUC: 0.9031, Accuracy: 0.8249\n",
            "Logistic Regression - AUC: 0.8754, Accuracy: 0.7895\n",
            "Gradient-Boosted Trees - AUC: 0.8949, Accuracy: 0.8186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper parameter tuning\n",
        "# Create Random Forest, Logistic Regression, and Gradient-Boosted Trees models\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Create a parameter grid for hyperparameter tuning for Random Forest\n",
        "rf_paramGrid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(rf.numTrees, [10, 50, 100])\n",
        "    .addGrid(rf.maxDepth, [5, 10, 15])\n",
        "    .addGrid(rf.impurity, [\"gini\", \"entropy\"])\n",
        "    .build()\n",
        ")\n",
        "\n",
        "\n",
        "# Create a cross-validator for model selection and hyperparameter tuning\n",
        "rf_crossval = CrossValidator(\n",
        "    estimator=rf,\n",
        "    estimatorParamMaps=rf_paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=5,\n",
        ")\n",
        "\n",
        "# Train the model using the training data and find the best hyperparameters\n",
        "rf_cv_model = rf_crossval.fit(prepared_train_data)\n",
        "best_rf = rf_cv_model.bestModel\n",
        "\n",
        "print(\"Best NumTrees:\", best_rf.getNumTrees)\n",
        "print(\"Best MaxDepth:\", best_rf.getOrDefault(best_rf.getParam(\"maxDepth\")))\n",
        "print(\"Best Impurity:\", best_rf.getOrDefault(best_rf.getParam(\"impurity\")))\n",
        "\n",
        "\n",
        "rf_predictions = best_rf.transform(prepared_test_data).withColumn(\"rf_prediction\", col(\"prediction\"))\n",
        "\n",
        "# Evaluate the model performance\n",
        "rf_auc = evaluator.evaluate(rf_predictions)\n",
        "rf_accuracy = predictions.filter(rf_predictions.label == rf_predictions.rf_prediction).count() / rf_predictions.count()\n",
        "\n",
        "# Evaluate the model performance\n",
        "print(f\"Random Forest (Tuned) - AUC: {rf_auc:.4f}, Accuracy: {rf_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "NcbTGTyZ4Fmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Logistic Regression \n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Create parameter grids for hyperparameter tuning\n",
        "lr_paramGrid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(lr.regParam, [0.01, 0.1, 1.0])\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "    .build()\n",
        ")\n",
        "\n",
        "\n",
        "# Create cross-validators for model selection and hyperparameter tuning\n",
        "lr_crossval = CrossValidator(\n",
        "    estimator=lr,\n",
        "    estimatorParamMaps=lr_paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=5,\n",
        ")\n",
        "\n",
        "# Train the models using the training data and find the best hyperparameters\n",
        "lr_cv_model = lr_crossval.fit(prepared_train_data)\n",
        "\n",
        "# Get the best models and print the best hyperparameters\n",
        "best_lr = lr_cv_model.bestModel\n",
        "\n",
        "print(\"Best Logistic Regression Parameters:\")\n",
        "print(\"Best RegParam:\", best_lr.getOrDefault(best_lr.getParam(\"regParam\")))\n",
        "print(\"Best ElasticNetParam:\", best_lr.getOrDefault(best_lr.getParam(\"elasticNetParam\")))\n",
        "\n",
        "# Make predictions using the testing data\n",
        "lr_predictions = best_lr.transform(prepared_test_data).withColumn(\"lr_prediction\", col(\"prediction\"))\n",
        "\n",
        "# Evaluate the model performance\n",
        "lr_auc = evaluator.evaluate(lr_predictions)\n",
        "lr_accuracy = lr_predictions.filter(lr_predictions.label == lr_predictions.lr_prediction).count() / lr_predictions.count()\n",
        "\n",
        "print(f\"\\nLogistic Regression (Tuned) - AUC: {lr_auc:.4f}, Accuracy: {lr_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "sWAP_6344GlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create gradient boosted tree\n",
        "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Create parameter grids for hyperparameter tuning\n",
        "gbt_paramGrid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(gbt.maxIter, [10, 50, 100])\n",
        "    .addGrid(gbt.maxDepth, [3, 5, 7])\n",
        "    .addGrid(gbt.stepSize, [0.1, 0.5, 1.0])\n",
        "    .build()\n",
        ")\n",
        "\n",
        "# Create cross-validators for model selection and hyperparameter tuning\n",
        "\n",
        "gbt_crossval = CrossValidator(\n",
        "    estimator=gbt,\n",
        "    estimatorParamMaps=gbt_paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=5,\n",
        ")\n",
        "\n",
        "# Train the models using the training data and find the best hyperparameters\n",
        "gbt_cv_model = gbt_crossval.fit(prepared_train_data)\n",
        "\n",
        "# Get the best models and print the best hyperparameters\n",
        "best_gbt = gbt_cv_model.bestModel\n",
        "\n",
        "print(\"\\nBest Gradient-Boosted Trees Parameters:\")\n",
        "print(\"Best MaxIter:\", best_gbt.getOrDefault(best_gbt.getParam(\"maxIter\")))\n",
        "print(\"Best MaxDepth:\", best_gbt.getOrDefault(best_gbt.getParam(\"maxDepth\")))\n",
        "print(\"Best StepSize:\", best_gbt.getOrDefault(best_gbt.getParam(\"stepSize\")))\n",
        "\n",
        "# Make predictions using the testing data\n",
        "gbt_predictions = best_gbt.transform(prepared_test_data).withColumn(\"gbt_prediction\", col(\"prediction\"))\n",
        "\n",
        "# Evaluate the model performance\n",
        "gbt_auc = evaluator.evaluate(gbt_predictions)\n",
        "gbt_accuracy = gbt_predictions.filter(gbt_predictions.label == gbt_predictions.gbt_prediction).count() / gbt_predictions.count()\n",
        "\n",
        "print(f\"Gradient-Boosted Trees (Tuned) - AUC: {gbt_auc:.4f}, Accuracy: {gbt_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "xDf8wt734HRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model evaluation (2m)\n",
        "\n",
        "# For this task, I will be making a new test data from previous train and test data \n",
        "# this is to avoid overfitting of test data during hyperparameter tuningm\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Combine the train and test datasets\n",
        "all_data = bank_train.union(bank_test)\n",
        "\n",
        "# Split the data into train and test\n",
        "train_data, new_test_data = all_data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Apply the same data preparation steps as before\n",
        "\n",
        "prepared_new_test_data = pipeline.fit(new_test_data).transform(new_test_data)\n",
        "prepared_test_data = selector_model.transform(prepared_test_data)\n",
        "\n",
        "\n",
        "# Create evaluators for F1 score, precision, and recall\n",
        "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "\n",
        "# Evaluate the models with the new test dataset\n",
        "best_models = {'Logistic Regression': best_lr, 'Random Forest': best_rf, 'Gradient-Boosted Trees': best_gbt}\n",
        "for name, model in best_models.items():\n",
        "    predictions = model.transform(prepared_new_test_data)\n",
        "    \n",
        "    auc = evaluator.evaluate(predictions)\n",
        "    accuracy = predictions.filter(predictions.label == predictions.prediction).count() / predictions.count()\n",
        "    f1 = f1_evaluator.evaluate(predictions)\n",
        "    precision = precision_evaluator.evaluate(predictions)\n",
        "    recall = recall_evaluator.evaluate(predictions)\n",
        "    \n",
        "    print(f\"{name} - AUC: {auc:.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZYTCRALv5iyF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}